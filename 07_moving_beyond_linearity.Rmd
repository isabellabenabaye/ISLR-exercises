---
title: "ISLR, Chapter 7 - Moving Beyond Linearity"
output: github_document
date: May 18, 2020
---

Here we will explore nonlinear models using the `Wage` dataset from the `ISLR` package.

# Polynomial Regression

First we will use polynomials and focus on a single predictor `age`:

```{r}
library(ISLR)

fit_p4 = lm(wage ~ poly(age,4), data = Wage) 

summary(fit_p4)
```

The first two polynomials are very significant, the cubic as well, but the quadric is just barely significant. A cubic polynomial woul probably be sufficient.

`poly()` generates orthogonal polynomials-- the predictors are uncorrelated.

We aren't interested in the coefficients, but instead the function that it has produced. Let's look at it:

```{r}
## get the range of the ages
agelims = range(Wage$age)
## make a grid of values of age using seq()
age_grid = seq(from = agelims[1], to = agelims[2])

# Make predictions, inclue the standard errors
## predictions will be a list of two components: the predicted value and the SE
predictions = predict(fit_p4, newdata = list(age = age_grid), se = TRUE)

# Make standard error bands
se_bands = cbind(predictions$fit + 2*predictions$se.fit, predictions$fit - 2*predictions$se.fit)

# Plot
plot(Wage$age, Wage$wage, col = "darkgray")
lines(age_grid, predictions$fit, col = "blue")
matlines(age_grid, se_bands, col = "blue", lty = 2) ## lty = 2, broken line
```

If we were to do the polynomial regression manually using `I(age^2)` etc, we would get different p-values, but the same fitted values.

By using orthogonal polynomials (used in `poly()`), it turns out that we can separately test for each coefficient using the p-values to determine which degrees are not needed. So if we look at the summary again, we can see that the linear, quadratic, and cubic terms are significant, but not the quartic.

This idea of testing separately only works when that's the only term in the model and if it's a glm.

Generally, if you wanted to test whether one degree is better than another, you would use `anova()` and a nested sequence of models.

```{r}
# A nested (in complexity) sequence of models
fita = lm(wage ~ education, data = Wage)
fitb = lm(wage ~ education + age, data = Wage)
fitc = lm(wage ~ education + poly(age,2), data = Wage)
fitd = lm(wage ~ education + poly(age,3), data = Wage)

# To see which model is best, we'll use anova()
anova(fita, fitb, fitc, fitd)
```

Based on the output of the anova, certainly age & age^2 are needed in the model with education, but age^3 is not necessarily as important.

# Polynomial Logistic Regression

We'll create a binary response to be fit using logistic regression.

Wage will be divided between big earners (>250k) and smaller earners.

```{r}
fit = glm(I(wage > 250) ~ poly(age,3), data = Wage, family = "binomial")
summary(fit)
```

Now, because it's a glm, even though the polynomial had an orthogonal basis, it's no longer strictly orthogonal when you fit a glm because it involves having weights for the observations so the orthogonality is somewhat lost. If we really wanted to test if a polynomial of degree 3 was needed, we'd have to fit another glm with polynomial of degree 2 and test to see if the two were different.

We'll get and plot the SE like in the previous example:

```{r}
predictions = predict(fit, newdata = list(age = age_grid), se = TRUE)

# Make standard error bands
se_bands = predictions$fit + cbind(fit = 0, lower = -2*predictions$se.fit, upper = 2*predictions$se.fit)

se_bands[1:5,]
```

We have done the computations of the model are done on the logit scale. We're more interested in the predictions on the probability scale, especially when fitting against a single variable. To transform to the probability scale, we need to apply the inverse logit mapping to both the fitted function and the SE bands:

$$p=\frac{e^\eta}{1+e^\eta}.$$

```{r}
prob_bands = exp(se_bands)/(1 + exp(se_bands))

matplot(age_grid, prob_bands, col = "blue", lwd = c(2,1,1), lty = c(1,2,2), type = "l", ylim = c(0,.1))
points(jitter(Wage$age), I(Wage$wage > 250)/10, pch = "|", cex = 0.5)
```

The rugs show the density of the points at each age. 

Only 4% at most of the population in any given age category earned above $250k.

# Cubic Splines (regression)

Splines are more flexible than polynomials, but the idea is rather similar.

First, we'll fit a cubic spline with knots at ages 25, 40, and 60.

```{r}
library(splines)

fit = lm(wage ~ bs(age, knots = c(25,40,60)), data = Wage)
plot(Wage$age, Wage$wage, col = "darkgray")
lines(age_grid, predict(fit, list(age = age_grid)), col = "darkgreen", lwd = 2)
## marking the knots - places of discontinuity
abline(v = c(25,40,60), lty = 2, col = "darkgreen")
```

# Smoothing Splines

Smoothing splines don't require knot selection, and instead have a smoothing parameter, which an be specified via the effective degrees of freedom, `df`.

```{r}
## smooth.spline() doesn't use formula language
fit = smooth.spline(Wage$age, Wage$wage, df = 16)
lines(fit, col = "red", lwd = 2)
```

We can also use leave one out (LOO) cross validation to select the smoothing parameter for us automatically.

```{r}
fit = smooth.spline(Wage$age, Wage$wage, cv = TRUE)
lines(fit, col = "blue", lwd = 2)
fit
```

The effective degrees of freedom that the CV approach chose was 6.79. Effective degrees of freedom is euristic for how rough the function is. Our fixed-knot regression spline had 6 degrees of freedom. 

# Generalized Additive Models (GAM)

So far we've been fitting models with mostly single nonlinear terms. With GAMs we can work with multiple nonlinear terms. 

The `gam` package also knows how to plot the GAM functions and their standard errors.

```{r}
library(gam)
## s() for a smoothing spline
gam = gam (wage ~ s(age, df = 4) + s(year, df = 4) + education, data = Wage)

# Set up the plotting grid with 3 columns
par(mfrow = c(1,3))

plot(gam, se = T)
```

The plot() function produces a plot for each of the terms in the gam, plotting them and their SEs. The plot of age is also what we've been seeing in the previous models. We see that salary tends to increase with the year, with a dip around 2006. And not surprisingly, salary increases monotonically with education.

Now we'll try it with our binary variable:

```{r}
gam_bin = gam (I(wage > 250) ~ s(age, df = 4) + s(year, df = 4) + education, data = Wage, family = binomial)

plot(gam_bin)
```

This plot plots the contributions of the predictors to the logit of the probability, each as separate functions. 

We can also test if we need a nonlinear term for year:

```{r}
gam_bin2 = gam (I(wage > 250) ~ s(age, df = 4) + year + education, data = Wage, family = binomial)

anova(gam_bin2, gam_bin, test = "Chisq")
```

Since the p-value of chi-sq test is not significant, it indicates that we really don't need the nonlinear term for year.

`gam` also knows how to plot the functions nicely for models fit by `lm` and `glm`.

```{r}
par(mfrow = c(1,3))
# ns() for natural splines; we specify the effective degrees of freedom
lm1 = lm(wage ~ ns(age, df = 4) + ns(year, df = 4) + education, data = Wage)

plot.Gam(lm1, se = T)
```

