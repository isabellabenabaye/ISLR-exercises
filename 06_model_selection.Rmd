---
title: "ISLR, Chapter 6 - Linear Model Selection and Regularization"
date: May 16 - 17, 2020
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r import libraries}
library(ISLR)
library(tidyverse)
```

We will use the `Hitters` database, a baseball database that has statistics for different baseball players, including the variable `Salary`, which we will be using as the response variable.

Looking at it:
```{r}
summary(Hitters)
```

`Salary` has 59 null values. To deal with this we will drop those observations.

```{r}
Hitters <- drop_na(Hitters)

# Double check if there are any remaining NAs
sum(is.na(Hitters$Salary))
```

# Best Subset Regression

Best Subest Regression looks through all possible regression models of all different subset sizes, gets the best of each size, then chooses the best one.

The library {leaps} can be used to evaluate the best subset models using the function `regsubsets()`. By default it only goes up to subsets of size 8, but in this dataset we have 19 variables and since we're doing BSR, we want 19 subsets. We will define that using `nvmax = `

```{r BSR}
library(leaps)
regfit_full = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)

# Get the summary
regfit_summary = summary(regfit_full)

# Look at the metrics in the summary contains for each of the best subset models
names(regfit_summary)

# Cp - an estimate of prediction error
regfit_summary$cp

# Pick the model with the lowest Cp, which we can check by plotting the Cps
plot(regfit_summary$cp, xlab = "Number of Variables", ylab = "Cp")
# The model with the lowest Cp is the one with 10 variables. We can also check this using the `which()` function:
which.min(regfit_summary$cp)
# Adding a marker to the plot to indicate it
points(10,regfit_summary$cp[10], pch = 20, col = "red")
```

Interpreting the summary:
Rows - subset size (number of predictors in the subset)
Value/stars - whether that variable is in the model

The model with the lowest Cp is the one with 10 variables. 

We can also plot the `regsubsets` object to get a summary of all the models, instead of just seeing the Cp statistic:
```{r}
plot(regfit_full, scale = "Cp") 
```

Interpretation: As expected, the model with the lowest Cp is at the top, the black squares indicate the variable is in the model, and we can see at first the models are reasonably stable, many of the variables stay the same with a little bit of fluctuation. We can also see that the models with the worst Cps are the ones with either too many or too little variables.

Now that we choose the model with 10 variables, we can get the coefficents for that model.
```{r}
coef(regfit_full,10)
```

# Forward Stepwise Selection

Forward Stepwise Selection produces nested sequences. It can be done using the same function `regsubsets`, where you specify the method as being "forward".

```{r}
regfit_forward = regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit_forward)
plot(regfit_forward, scale = "Cp")
```

The structure looks similar to the one made by the best subset selection, especially with the better models.

Instead of using Cp, the adjusted R squared, and the other metrics provided by the function that are in the summary, we will use a Validation set to choose the best model.

# Model Selection - Using a Validation Set

We will make a training and validation set so that we can choose a good subset model. This will be a different approach from what is done in the book.

Then we'll fit another model using `regsubsets()` again but only using the training set (taken using the indexes gotten using `sample()`).

```{r}
# Look at the dimensions of the table
dim(Hitters)
## there are 263 observations and 20 variables-- we can do 2/3 (~180 obs) training 1/3 test

set.seed(1)

# Get a sample of 180 indexes
train = sample(seq(263),180,replace = FALSE)
train
regfit_forward = regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19, method = "forward")
```


Now we can make predictions on the observations used for training (testing data).

We know there are 19 models so we have to set up some vectors to record the errors, and we have to do work here because there is no predict method for `regsubsets`.

```{r}
# Set up a vector having 19 slots
val_errors = rep(NA, 19)

# Make validation dataset
X_test = model.matrix(Salary ~ ., data = Hitters[-train,]) ## -index to get the non-training data

# Make predictions for each model
for (i in  1:19) {
  coefi = coef(regfit_forward, id = i)
  pred = X_test[,names(coefi)]%*%coefi
  val_errors[i] = mean((Hitters$Salary[-train] - pred)^2)
}

# Plot the root MSE (validation) and RSS (training)
  plot(sqrt(val_errors), ylab = "Root MSE",ylim = c(300,410), pch = 19, type = "b")
  points(sqrt(regfit_forward$rss[-1]/180), col = "blue", pch = 19, type = "b")
```

Now we'll write a predict method function for `regsubsets`:

```{r}
predict_regsubsets = function(object, newdata, id, ....) {
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[, names(coefi)]%*%coefi
}
```

# Model Selection - Using Cross-Validation

10-fold cross-validation:
```{r}
set.seed(11)

# Assign each observation a fold number (1-10), randomly assigned using sample()
folds = sample(rep(1:10, length = nrow(Hitters)))
folds
# See how balanced the distribution of the observations is to each fold by tabulating the counts
table(folds)

# Make a matrix for the errors 10 rows (one for each of the 10 folds), 19 columns (19 variables = 19 subsets)
cv_errors = matrix(NA, 10, 19)
cv_errors


for (k in 1:10) {
  # Train on all subsets != k, leaving out the k fold to be used for testing
  best_fit = regsubsets(Salary ~ ., data = Hitters[folds != k,], nvmax = 19, method = "forward")
  
  # Look at each of the subsets in the trained model
  for (i in 1:19) {
    # Use the predict method we just made 
    # best_fit is our regsubsets object
    # make predictions using the k fold (testing set)
    pred = predict_regsubsets(best_fit, Hitters[folds == k,], id = i)
    # Put MSE of predictions into the k-th row of cv_errors
    # 10 rows (folds), 19 columns (subsets/models)
    cv_errors[k,i] = mean( (Hitters$Salary[folds == k] - pred)^2 )
  }
}

# Process the output matrix
# apply function to the columns to get the column means (mean of all the 10 folds' MSE per subset)
# sqrt() to get the RMSE
rmse_cv = sqrt(apply(cv_errors, 2, mean))

# Plot the RMSE
plot(rmse_cv, pch = 19, type = "b")
```

# Ridge Regression

To fit a ridge regression model, we'll use the `glmnet` package, which does not use the model formula language, so we will set up on `x` and `y`.

`glmnet()` fits models on a whole range of lambda values

Ridge regression - alpha = 0 (keeps all predictors)
Lasso - alpha = 1 (does shrinkage & variable selection)
Elastic net models - alpha = (0,1)

`cv.glmnet` does cross validation for you.

```{r}
library(glmnet)
X = model.matrix(Salary ~ .-1, data = Hitters)
y = Hitters$Salary

# Ridge regression
fit_ridge = glmnet(X, y, alpha = 0)
plot(fit_ridge, xvar = "lambda", label = TRUE)
```

The numebers at the top indicate how many variables (predictors + intercept) are in the models/stages.

The far left of the plot shows the coefficients of the least quares fit where lambda = 0 & coeffficients are unregularized.

Now we need to pick a value of lambda, which we can use `cv.glmnet` for. The default is 10-fold cross-validation.

```{r}
cv_ridge = cv.glmnet(X, y, alpha = 0)
plot(cv_ridge)
```

The plot seems to indicate that the full model is actually doing a pretty good job. 

The first dashed line indicates the minimum MSE, and the second line is at one standard error of the minimum-- a slightly more restricted model that does almost as well as the minimum.

# Lasso Model

```{r}
# Lasso model
fit_lasso = glmnet(X, y, alpha = 1)
plot(fit_lasso, xvar = "lambda", label = TRUE)

# We can also plot the percentage of deviance explained/in regression, R^2
plot(fit_lasso, xvar = "dev", label = TRUE)
```

In the lasso lambda plot, the top still indicates how many variables are in the model, and since lasso makes some coefficients 0, we can see the when they're removed.

In the deviance explained plot, the right hand side where the coefficients are large may indicate overfitting.

```{r}
cv_lasso = cv.glmnet(X, y, alpha = 1)
plot(cv_lasso)
```

`fit_lasso` contains the whole path of coefficients-- for all values of lambda that it computed (~100 coeff vectors indexed by different values of lambda), but if you call `coef()` on the object created by `cv.glmnet()`, in this case `cv_lasso`, it will only return the coefficient vector corresponding to the best model.

```{r}
coef(cv_lasso)
```

In this case, it's the model of 5 predictors, the one one SD away from the minimum MSE (more parsimonious- the CV error is measured with some variance).

Now we'll try using the earlier training/validation sets to select the `lambda` for the lasso.

```{r}
lasso_training = glmnet(X[train,], y[train])
lasso_training
## the glmnet object contains the degrees of freedom, percentage of deviance explained (like R^2 for glms), and the lambda used to train that model/to that fit

pred = predict(lasso_training, X[-train,])
dim(pred)
## there are 83 observations in the validation set and 83 values of lambda

rmse = sqrt(apply( (y[-train] - pred)^2, 2, mean))

# Plot the RMSEs of the different models
plot(log(lasso_training$lambda), rmse, type = "b", xlab = "Log(lambda)")
## left - overfitting, right - underfitting, middle - sweet spot

# Extract the best lambda
lam_best = lasso_training$lambda[order(rmse)[1]]
lam_best

# Get the coefficients of that model
coef(lasso_training, s = lam_best)
```

